# H∆∞·ªõng D·∫´n Deep Learning Cho D·ª± B√°o Ranking C·∫ßu L√¥ng BWF

## üìö M·ª•c L·ª•c
1. [Deep Learning l√† g√¨?](#1-deep-learning-l√†-g√¨)
2. [T·∫°i sao s·ª≠ d·ª•ng DL cho BWF Ranking?](#2-t·∫°i-sao-s·ª≠-d·ª•ng-dl-cho-bwf-ranking)
3. [C√°c ki·∫øn tr√∫c DL ph√π h·ª£p](#3-c√°c-ki·∫øn-tr√∫c-dl-ph√π-h·ª£p)
4. [D·ªØ li·ªáu BWF v√† Feature Engineering](#4-d·ªØ-li·ªáu-bwf-v√†-feature-engineering)
5. [Implementation v·ªõi PyTorch/TensorFlow](#5-implementation-v·ªõi-pytorchtensorflow)
6. [So s√°nh DL vs ML truy·ªÅn th·ªëng](#6-so-s√°nh-dl-vs-ml-truy·ªÅn-th·ªëng)
7. [Best Practices v√† Tips](#7-best-practices-v√†-tips)

---

## 1. Deep Learning l√† g√¨?

### 1.1 ƒê·ªãnh nghƒ©a
**Deep Learning (DL)** l√† m·ªôt nh√°nh con c·ªßa Machine Learning, s·ª≠ d·ª•ng m·∫°ng neural nh√¢n t·∫°o (Artificial Neural Networks) v·ªõi nhi·ªÅu t·∫ßng ·∫©n (hidden layers) ƒë·ªÉ h·ªçc c√°c ƒë·∫∑c tr∆∞ng ph·ª©c t·∫°p t·ª´ d·ªØ li·ªáu.

### 1.2 C·∫•u tr√∫c c∆° b·∫£n c·ªßa Neural Network

```
Input Layer ‚Üí Hidden Layer 1 ‚Üí Hidden Layer 2 ‚Üí ... ‚Üí Output Layer
```

**V√≠ d·ª• v·ªõi d·ªØ li·ªáu BWF:**
```
[Features: points, rank, win_rate, ...] 
    ‚Üì
[Hidden Layer 1: 128 neurons] 
    ‚Üì
[Hidden Layer 2: 64 neurons]
    ‚Üì
[Hidden Layer 3: 32 neurons]
    ‚Üì
[Output: Predicted ranking/points]
```

### 1.3 C√°c th√†nh ph·∫ßn ch√≠nh

#### a) Neurons (N∆°-ron)
- ƒê∆°n v·ªã x·ª≠ l√Ω c∆° b·∫£n
- Nh·∫≠n input, √°p d·ª•ng weights v√† bias
- K·∫øt qu·∫£ qua activation function

#### b) Weights v√† Biases
- **Weights (tr·ªçng s·ªë)**: ƒê·ªô quan tr·ªçng c·ªßa m·ªói k·∫øt n·ªëi
- **Biases (ƒë·ªô l·ªách)**: ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng k√≠ch ho·∫°t
- ƒê∆∞·ª£c h·ªçc t·ª± ƒë·ªông qua qu√° tr√¨nh training

#### c) Activation Functions
- **ReLU**: `f(x) = max(0, x)` - Ph·ªï bi·∫øn nh·∫•t
- **Sigmoid**: `f(x) = 1/(1+e^-x)` - Cho output 0-1
- **Tanh**: `f(x) = (e^x - e^-x)/(e^x + e^-x)` - Output -1 ƒë·∫øn 1

---

## 2. T·∫°i sao s·ª≠ d·ª•ng DL cho BWF Ranking?

### 2.1 ∆Øu ƒëi·ªÉm c·ªßa DL

‚úÖ **Automatic Feature Learning**
- DL t·ª± ƒë·ªông h·ªçc c√°c pattern ph·ª©c t·∫°p
- Kh√¥ng c·∫ßn thi·∫øt k·∫ø features th·ªß c√¥ng nhi·ªÅu
- Ph√°t hi·ªán ƒë∆∞·ª£c c√°c m·ªëi quan h·ªá phi tuy·∫øn

‚úÖ **Temporal Dependencies**
- X·ª≠ l√Ω t·ªët chu·ªói th·ªùi gian (time series)
- H·ªçc ƒë∆∞·ª£c xu h∆∞·ªõng d√†i h·∫°n v√† ng·∫Øn h·∫°n
- Ph√π h·ª£p v·ªõi d·ªØ li·ªáu ranking theo th·ªùi gian

‚úÖ **Multiple Draws Handling**
- C√≥ th·ªÉ h·ªçc ƒë·∫∑c tr∆∞ng ri√™ng cho t·ª´ng draw (WS, MS, WD, MD, XD)
- Transfer learning gi·ªØa c√°c draws

### 2.2 Khi n√†o N√äN d√πng DL?

‚úîÔ∏è C√≥ nhi·ªÅu d·ªØ li·ªáu (>10,000 samples)
‚úîÔ∏è Pattern ph·ª©c t·∫°p, phi tuy·∫øn
‚úîÔ∏è C·∫ßn d·ª± b√°o chu·ªói th·ªùi gian
‚úîÔ∏è Mu·ªën t·ª± ƒë·ªông feature engineering

### 2.3 Khi n√†o KH√îNG N√äN d√πng DL?

‚ùå D·ªØ li·ªáu √≠t (<1,000 samples)
‚ùå C·∫ßn interpretability cao (gi·∫£i th√≠ch t·ª´ng quy·∫øt ƒë·ªãnh)
‚ùå T√†i nguy√™n t√≠nh to√°n h·∫°n ch·∫ø
‚ùå ML truy·ªÅn th·ªëng ƒë√£ cho k·∫øt qu·∫£ t·ªët

---

## 3. C√°c ki·∫øn tr√∫c DL ph√π h·ª£p

### 3.1 Feedforward Neural Network (FNN)

**Khi n√†o d√πng:** D·ª± b√°o ƒëi·ªÉm/ranking t·∫°i th·ªùi ƒëi·ªÉm hi·ªán t·∫°i

```python
class RankingFNN(nn.Module):
    def __init__(self, input_size, hidden_sizes=[128, 64, 32]):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_sizes[0])
        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])
        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])
        self.output = nn.Linear(hidden_sizes[2], 1)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.dropout(self.relu(self.fc2(x)))
        x = self.relu(self.fc3(x))
        return self.output(x)
```

**∆Øu ƒëi·ªÉm:**
- ƒê∆°n gi·∫£n, d·ªÖ implement
- Training nhanh
- Ph√π h·ª£p v·ªõi tabular data

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Kh√¥ng x·ª≠ l√Ω t·ªët temporal dependencies
- C·∫ßn feature engineering k·ªπ

---

### 3.2 LSTM (Long Short-Term Memory)

**Khi n√†o d√πng:** D·ª± b√°o xu h∆∞·ªõng ranking theo th·ªùi gian

```python
class RankingLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2
        )
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        # x shape: (batch, sequence_length, features)
        lstm_out, _ = self.lstm(x)
        # L·∫•y output c·ªßa timestep cu·ªëi c√πng
        last_output = lstm_out[:, -1, :]
        return self.fc(last_output)
```

**∆Øu ƒëi·ªÉm:**
- X·ª≠ l√Ω t·ªët sequential data
- Nh·ªõ ƒë∆∞·ª£c th√¥ng tin d√†i h·∫°n
- Ph√π h·ª£p v·ªõi time series

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Training ch·∫≠m h∆°n FNN
- C·∫ßn nhi·ªÅu d·ªØ li·ªáu h∆°n
- C√≥ th·ªÉ overfit

**C√°ch s·ª≠ d·ª•ng v·ªõi d·ªØ li·ªáu BWF:**
```python
# T·∫°o sequences t·ª´ d·ªØ li·ªáu
# VD: D√πng 12 tu·∫ßn tr∆∞·ªõc ƒë·ªÉ d·ª± b√°o tu·∫ßn ti·∫øp theo
sequence_length = 12
features = ['points', 'rank', 'win_rate_estimated', 'momentum_score', ...]

# Input shape: (batch_size, 12, num_features)
# Output: Predicted points/rank cho tu·∫ßn ti·∫øp theo
```

---

### 3.3 Transformer

**Khi n√†o d√πng:** B√†i to√°n ph·ª©c t·∫°p, c·∫ßn attention mechanism

```python
class RankingTransformer(nn.Module):
    def __init__(self, input_size, d_model=64, nhead=4, num_layers=2):
        super().__init__()
        self.embedding = nn.Linear(input_size, d_model)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,
            dim_feedforward=256
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=num_layers
        )
        self.fc = nn.Linear(d_model, 1)
    
    def forward(self, x):
        # x: (batch, seq_len, features)
        x = self.embedding(x)  # (batch, seq_len, d_model)
        x = x.transpose(0, 1)  # (seq_len, batch, d_model)
        x = self.transformer(x)
        x = x.mean(dim=0)  # Global average pooling
        return self.fc(x)
```

**∆Øu ƒëi·ªÉm:**
- State-of-the-art performance
- Attention mechanism h·ªçc ƒë∆∞·ª£c quan h·ªá quan tr·ªçng
- Parallel processing (nhanh h∆°n LSTM)

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Ph·ª©c t·∫°p, kh√≥ tune
- C·∫ßn nhi·ªÅu d·ªØ li·ªáu
- T√†i nguy√™n t√≠nh to√°n cao

---

## 4. D·ªØ li·ªáu BWF v√† Feature Engineering

### 4.1 Ph√¢n t√≠ch d·ªØ li·ªáu bwf_official_enhanced.csv

T·ª´ d·ªØ li·ªáu c·ªßa b·∫°n, t√¥i th·∫•y c√°c features sau:

#### A. Temporal Features (Th·ªùi gian)
```python
temporal_features = [
    'career_years',      # S·ªë nƒÉm trong s·ª± nghi·ªáp
    'career_months',     # S·ªë th√°ng
    'in_peak_career',    # C√≥ ƒëang ·ªü ƒë·ªânh cao kh√¥ng (0/1)
    'career_stage'       # Giai ƒëo·∫°n: rookie/rising/peak/declining
]
```

#### B. Performance Features (Th√†nh t√≠ch)
```python
performance_features = [
    'points',                  # ƒêi·ªÉm hi·ªán t·∫°i
    'rank',                    # H·∫°ng hi·ªán t·∫°i
    'tournaments_played',      # S·ªë gi·∫£i ƒë·∫•u ƒë√£ ch∆°i
    'win_rate_estimated',      # T·ª∑ l·ªá th·∫Øng ∆∞·ªõc t√≠nh
    'loss_rate_estimated',     # T·ª∑ l·ªá thua
    'win_streak_estimated'     # Chu·ªói th·∫Øng li√™n ti·∫øp
]
```

#### C. Historical Features (L·ªãch s·ª≠)
```python
historical_features = [
    'points_lag_1',      # ƒêi·ªÉm 1 tu·∫ßn tr∆∞·ªõc
    'points_lag_3',      # ƒêi·ªÉm 3 tu·∫ßn tr∆∞·ªõc
    'points_lag_6',      # ƒêi·ªÉm 6 tu·∫ßn tr∆∞·ªõc
    'points_lag_12',     # ƒêi·ªÉm 12 tu·∫ßn tr∆∞·ªõc
    'rank_lag_1',        # H·∫°ng 1 tu·∫ßn tr∆∞·ªõc
    'rank_lag_3'         # H·∫°ng 3 tu·∫ßn tr∆∞·ªõc
]
```

#### D. Statistical Features (Th·ªëng k√™)
```python
statistical_features = [
    'avg_points_3m',       # ƒêi·ªÉm trung b√¨nh 3 th√°ng
    'avg_points_6m',       # ƒêi·ªÉm trung b√¨nh 6 th√°ng
    'avg_points_12m',      # ƒêi·ªÉm trung b√¨nh 12 th√°ng
    'std_points_6m',       # ƒê·ªô l·ªách chu·∫©n 6 th√°ng
    'std_points_12m',      # ƒê·ªô l·ªách chu·∫©n 12 th√°ng
    'points_change_1m',    # Thay ƒë·ªïi ƒëi·ªÉm 1 th√°ng
    'points_change_6m',    # Thay ƒë·ªïi ƒëi·ªÉm 6 th√°ng
    'rank_change_1m'       # Thay ƒë·ªïi h·∫°ng 1 th√°ng
]
```

#### E. Momentum Features (Xu h∆∞·ªõng)
```python
momentum_features = [
    'momentum_score',      # ƒêi·ªÉm momentum
    'trend_3m',            # Xu h∆∞·ªõng 3 th√°ng
    'consistency_score'    # ƒêi·ªÉm ·ªïn ƒë·ªãnh
]
```

### 4.2 Feature Engineering cho Deep Learning

```python
# 1. Normalization (Chu·∫©n h√≥a)
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Chu·∫©n h√≥a c√°c features v·ªÅ c√πng scale
scaler = StandardScaler()
normalized_features = scaler.fit_transform(df[all_features])

# 2. Categorical Encoding
# Draw: WS, MS, WD, MD, XD ‚Üí One-hot encoding
draw_encoded = pd.get_dummies(df['draw'], prefix='draw')

# Career stage ‚Üí Label encoding ho·∫∑c one-hot
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['career_stage_encoded'] = le.fit_transform(df['career_stage'])

# 3. Temporal Sequences (Cho LSTM/Transformer)
def create_sequences(df, sequence_length=12):
    """
    T·∫°o sequences cho m·ªói c·∫ßu th·ªß
    Input: DataFrame ƒë√£ sort theo player_id v√† date
    Output: X shape (n_samples, sequence_length, n_features)
            y shape (n_samples, 1)
    """
    sequences = []
    targets = []
    
    for player_id in df['player_id'].unique():
        player_data = df[df['player_id'] == player_id].sort_values('date')
        
        for i in range(len(player_data) - sequence_length):
            # L·∫•y 12 tu·∫ßn d·ªØ li·ªáu
            seq = player_data.iloc[i:i+sequence_length][features].values
            # Target: ƒëi·ªÉm c·ªßa tu·∫ßn th·ª© 13
            target = player_data.iloc[i+sequence_length]['points']
            
            sequences.append(seq)
            targets.append(target)
    
    return np.array(sequences), np.array(targets)
```

---

## 5. Implementation v·ªõi PyTorch/TensorFlow

### 5.1 Complete Training Pipeline (PyTorch)

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np

# ==================== Data Preparation ====================
class BWFDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y).unsqueeze(1)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# Load v√† prepare data
df = pd.read_csv('bwf_official_enhanced.csv')

# Select features
feature_cols = [
    'points', 'rank', 'tournaments_played', 'win_rate_estimated',
    'points_lag_1', 'points_lag_3', 'points_lag_6',
    'avg_points_3m', 'avg_points_6m', 'std_points_6m',
    'momentum_score', 'trend_3m', 'consistency_score',
    'career_years', 'career_months'
]

# Target: D·ª± b√°o ƒëi·ªÉm ·ªü tu·∫ßn ti·∫øp theo
X = df[feature_cols].values
y = df['points'].shift(-1).dropna().values  # Shift ƒë·ªÉ l·∫•y ƒëi·ªÉm tu·∫ßn sau
X = X[:-1]  # B·ªè d√≤ng cu·ªëi v√¨ kh√¥ng c√≥ label

# Train/Val/Test split
from sklearn.model_selection import train_test_split

X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.2, random_state=42
)

# Normalization
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

# Create DataLoaders
train_dataset = BWFDataset(X_train, y_train)
val_dataset = BWFDataset(X_val, y_val)
test_dataset = BWFDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64)
test_loader = DataLoader(test_dataset, batch_size=64)

# ==================== Model Definition ====================
class BWFRankingNet(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(32, 1)
        )
    
    def forward(self, x):
        return self.network(x)

# ==================== Training ====================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = BWFRankingNet(input_size=len(feature_cols)).to(device)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)

def train_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0
    
    for X_batch, y_batch in loader:
        X_batch = X_batch.to(device)
        y_batch = y_batch.to(device)
        
        # Forward pass
        predictions = model(X_batch)
        loss = criterion(predictions, y_batch)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(loader)

def validate(model, loader, criterion, device):
    model.eval()
    total_loss = 0
    
    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch = X_batch.to(device)
            y_batch = y_batch.to(device)
            
            predictions = model(X_batch)
            loss = criterion(predictions, y_batch)
            total_loss += loss.item()
    
    return total_loss / len(loader)

# Training loop
num_epochs = 100
best_val_loss = float('inf')

for epoch in range(num_epochs):
    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
    val_loss = validate(model, val_loader, criterion, device)
    
    scheduler.step(val_loss)
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pth')
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch {epoch+1}/{num_epochs}')
        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')

# ==================== Evaluation ====================
model.load_state_dict(torch.load('best_model.pth'))
test_loss = validate(model, test_loader, criterion, device)
print(f'\nTest Loss: {test_loss:.4f}')

# Get predictions
model.eval()
all_predictions = []
all_actuals = []

with torch.no_grad():
    for X_batch, y_batch in test_loader:
        predictions = model(X_batch.to(device))
        all_predictions.extend(predictions.cpu().numpy())
        all_actuals.extend(y_batch.numpy())

# Calculate metrics
from sklearn.metrics import mean_absolute_error, r2_score

mae = mean_absolute_error(all_actuals, all_predictions)
r2 = r2_score(all_actuals, all_predictions)

print(f'MAE: {mae:.2f} points')
print(f'R¬≤ Score: {r2:.4f}')
```

### 5.2 LSTM Implementation

```python
class BWFRankingLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2 if num_layers > 1 else 0
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1)
        )
    
    def forward(self, x):
        # x: (batch, sequence_length, features)
        lstm_out, (h_n, c_n) = self.lstm(x)
        # L·∫•y hidden state cu·ªëi c√πng
        last_hidden = h_n[-1]  # (batch, hidden_size)
        return self.fc(last_hidden)

# Chu·∫©n b·ªã data d·∫°ng sequences
X_seq, y_seq = create_sequences(df_sorted, sequence_length=12)
# X_seq shape: (n_samples, 12, n_features)
# y_seq shape: (n_samples,)

# Training t∆∞∆°ng t·ª± nh∆∞ FNN
```

---

## 6. So s√°nh DL vs ML truy·ªÅn th·ªëng

### 6.1 B·∫£ng so s√°nh

| Ti√™u ch√≠ | Deep Learning | ML Truy·ªÅn th·ªëng (RF, XGBoost) |
|----------|---------------|-------------------------------|
| **D·ªØ li·ªáu c·∫ßn** | Nhi·ªÅu (>10k samples) | √çt-Trung b√¨nh (>1k samples) |
| **Feature Engineering** | T·ª± ƒë·ªông h·ªçc features | C·∫ßn thi·∫øt k·∫ø th·ªß c√¥ng |
| **Interpretability** | Th·∫•p (black box) | Cao (feature importance) |
| **Training time** | Ch·∫≠m (gi·ªù-ng√†y) | Nhanh (ph√∫t-gi·ªù) |
| **Inference time** | Nhanh | R·∫•t nhanh |
| **Overfitting risk** | Cao | Trung b√¨nh |
| **Temporal patterns** | T·ªët (LSTM, Transformer) | C·∫ßn feature engineering |
| **Performance** | Cao (n·∫øu ƒë·ªß data) | T·ªët v·ªõi tabular data |

### 6.2 K·∫øt qu·∫£ th·ª±c t·∫ø v·ªõi d·ªØ li·ªáu BWF

D·ª±a tr√™n kinh nghi·ªám v√† d·ªØ li·ªáu c·ªßa b·∫°n:

**XGBoost/Random Forest:**
- ‚úÖ Cho k·∫øt qu·∫£ t·ªët v·ªõi ~29,000 samples
- ‚úÖ Training nhanh (< 1 ph√∫t)
- ‚úÖ D·ªÖ interpret (feature importance)
- ‚úÖ √çt prone to overfitting

**Deep Learning:**
- ‚úÖ C√≥ th·ªÉ h·ªçc temporal patterns t·ªët h∆°n
- ‚úÖ T·ª± ƒë·ªông feature interactions
- ‚ö†Ô∏è C·∫ßn tuning k·ªπ ƒë·ªÉ tr√°nh overfit
- ‚ö†Ô∏è Training l√¢u h∆°n

**Khuy·∫øn ngh·ªã:**
```
S·ª≠ d·ª•ng ENSEMBLE c·ªßa c·∫£ 2:
- XGBoost cho baseline t·ªët
- LSTM cho temporal patterns
- K·∫øt h·ª£p predictions (weighted average ho·∫∑c stacking)
```

---

## 7. Best Practices v√† Tips

### 7.1 Tr√°nh Overfitting

```python
# 1. Regularization
model = nn.Sequential(
    nn.Linear(input_size, 128),
    nn.Dropout(0.3),  # ‚Üê Dropout
    nn.Linear(128, 64),
    nn.Dropout(0.2)
)

# Weight decay trong optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

# 2. Early Stopping
class EarlyStopping:
    def __init__(self, patience=10):
        self.patience = patience
        self.counter = 0
        self.best_loss = None
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss:
            self.counter += 1
            if self.counter >= self.patience:
                return True
        else:
            self.best_loss = val_loss
            self.counter = 0
        return False

# 3. Data Augmentation (cho time series)
def augment_sequence(seq, noise_level=0.01):
    """Th√™m noise nh·∫π v√†o sequence"""
    noise = np.random.normal(0, noise_level, seq.shape)
    return seq + noise

# 4. Cross-validation
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True, random_state=42)
for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    X_train, X_val = X[train_idx], X[val_idx]
    # Train model for this fold
```

### 7.2 Hyperparameter Tuning

```python
# S·ª≠ d·ª•ng Optuna cho automatic tuning
import optuna

def objective(trial):
    # Suggest hyperparameters
    hidden_size_1 = trial.suggest_int('hidden_size_1', 64, 256)
    hidden_size_2 = trial.suggest_int('hidden_size_2', 32, 128)
    dropout_rate = trial.suggest_float('dropout', 0.1, 0.5)
    learning_rate = trial.suggest_loguniform('lr', 1e-5, 1e-2)
    
    # Build model
    model = nn.Sequential(
        nn.Linear(input_size, hidden_size_1),
        nn.ReLU(),
        nn.Dropout(dropout_rate),
        nn.Linear(hidden_size_1, hidden_size_2),
        nn.ReLU(),
        nn.Linear(hidden_size_2, 1)
    ).to(device)
    
    # Train and return validation loss
    # ... (training code)
    
    return val_loss

# Run optimization
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

print('Best hyperparameters:', study.best_params)
```

### 7.3 Monitoring v√† Visualization

```python
import matplotlib.pyplot as plt

# Track losses
train_losses = []
val_losses = []

# During training
for epoch in range(num_epochs):
    train_loss = train_epoch(...)
    val_loss = validate(...)
    
    train_losses.append(train_loss)
    val_losses.append(val_loss)

# Plot learning curves
plt.figure(figsize=(10, 6))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Learning Curves')
plt.savefig('learning_curves.png')

# Visualize predictions vs actual
plt.figure(figsize=(10, 6))
plt.scatter(all_actuals, all_predictions, alpha=0.5)
plt.plot([min(all_actuals), max(all_actuals)], 
         [min(all_actuals), max(all_actuals)], 
         'r--', label='Perfect prediction')
plt.xlabel('Actual Points')
plt.ylabel('Predicted Points')
plt.legend()
plt.title('Predictions vs Actual')
plt.savefig('predictions_vs_actual.png')
```

### 7.4 Tips cho BWF Ranking espec√≠ficamente

1. **Multi-task Learning**
```python
class MultiTaskModel(nn.Module):
    def __init__(self, input_size):
        super().__init__()
        # Shared layers
        self.shared = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        # Task-specific heads
        self.points_head = nn.Linear(128, 1)  # D·ª± b√°o ƒëi·ªÉm
        self.rank_head = nn.Linear(128, 1)    # D·ª± b√°o h·∫°ng
    
    def forward(self, x):
        shared_features = self.shared(x)
        points_pred = self.points_head(shared_features)
        rank_pred = self.rank_head(shared_features)
        return points_pred, rank_pred
```

2. **Draw-specific Models**
```python
# Train ri√™ng model cho m·ªói draw
models = {}
for draw in ['WS', 'MS', 'WD', 'MD', 'XD']:
    df_draw = df[df['draw'] == draw]
    models[draw] = train_model(df_draw)
    
# ho·∫∑c d√πng shared model v·ªõi draw embedding
```

3. **Ensemble Predictions**
```python
# K·∫øt h·ª£p DL v·ªõi XGBoost
dl_pred = model(X_test)
xgb_pred = xgb_model.predict(X_test)

# Weighted average
final_pred = 0.6 * dl_pred + 0.4 * xgb_pred

# Ho·∫∑c train meta-model (stacking)
```

---

## üìñ T√†i li·ªáu tham kh·∫£o

1. **PyTorch Official Tutorial**: https://pytorch.org/tutorials/
2. **Deep Learning Book** (Goodfellow et al.): https://www.deeplearningbook.org/
3. **Time Series Forecasting with Deep Learning**: 
   - https://arxiv.org/abs/1704.04110
4. **Practical Deep Learning for Coders** (fast.ai): https://course.fast.ai/

---

## üéØ K·∫øt lu·∫≠n

Deep Learning l√† c√¥ng c·ª• m·∫°nh m·∫Ω cho b√†i to√°n d·ª± b√°o BWF ranking, ƒë·∫∑c bi·ªát khi:
- B·∫°n c√≥ ƒë·ªß d·ªØ li·ªáu (‚úÖ ~29k samples)
- Mu·ªën t·ª± ƒë·ªông h·ªçc temporal patterns
- C·∫ßn d·ª± b√°o nhi·ªÅu draws kh√°c nhau

**L·ªô tr√¨nh ƒë·ªÅ xu·∫•t:**
1. ‚úÖ B·∫Øt ƒë·∫ßu v·ªõi FNN ƒë∆°n gi·∫£n
2. ‚úÖ Th·ª≠ LSTM cho temporal modeling
3. ‚úÖ So s√°nh v·ªõi XGBoost baseline
4. ‚úÖ Ensemble c√°c models l·∫°i
5. ‚úÖ Fine-tune hyperparameters

Ch√∫c b·∫°n th√†nh c√¥ng! üè∏
