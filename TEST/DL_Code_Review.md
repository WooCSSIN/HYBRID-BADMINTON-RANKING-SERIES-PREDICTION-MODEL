# üìä ƒê√°nh Gi√° Deep Learning Implementation - BWF Ranking Project

## üéØ T·ªïng Quan

**File ƒë∆∞·ª£c review:** `3_ensemble_models.py`  
**Model:** LSTM + LightGBM Ensemble  
**Framework:** PyTorch  
**ƒê√°nh gi√° t·ªïng th·ªÉ:** ‚≠ê‚≠ê‚≠ê‚≠ê (4/5) - **T·ªët, c√≥ ti·ªÅm nƒÉng c·∫£i thi·ªán**

---

## ‚úÖ ƒêI·ªÇM M·∫†NH (Strengths)

### 1. üèóÔ∏è **Ki·∫øn tr√∫c t·ªët - Well-designed Architecture**

#### a) Ensemble Approach ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```python
# K·∫øt h·ª£p 2 models complementary
- LightGBM: T·ªët v·ªõi tabular features (60% weight)
- LSTM: H·ªçc temporal patterns (40% weight)
```

**T·∫°i sao t·ªët:**
- ‚úÖ LightGBM x·ª≠ l√Ω t·ªët static features, LSTM b·∫Øt patterns theo th·ªùi gian
- ‚úÖ Weights (0.6/0.4) h·ª£p l√Ω - ∆∞u ti√™n model proven h∆°n
- ‚úÖ Fallback gracefully n·∫øu PyTorch kh√¥ng available

#### b) LSTM Architecture ‚≠ê‚≠ê‚≠ê‚≠ê
```python
self.lstm = nn.LSTM(
    input_size=input_size,
    hidden_size=64,        # ‚úÖ Reasonable size
    num_layers=2,          # ‚úÖ 2 layers - not too deep
    dropout=0.2,           # ‚úÖ Regularization included
    batch_first=True       # ‚úÖ Easy to work with
)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ 2-layer LSTM v·ªõi dropout - tr√°nh overfitting t·ªët
- ‚úÖ Hidden size 64 - ph√π h·ª£p v·ªõi ~13 features
- ‚úÖ `batch_first=True` - code d·ªÖ ƒë·ªçc h∆°n

---

### 2. üìê **Data Preparation Excellence**

#### a) Sequence Generation ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```python
def prepare_sequences(self, df, feature_cols, target_col='points'):
    # Group by player and draw
    for (player_id, draw), group in df.groupby(['player_id', 'draw']):
        # Create 12-month sequences
        for i in range(len(group) - self.sequence_length):
            seq = group.iloc[i:i+self.sequence_length][feature_cols].values
            target = group.iloc[i+self.sequence_length][target_col]
```

**ƒêi·ªÉm xu·∫•t s·∫Øc:**
- ‚úÖ Correctly grouped by `player_id` v√† `draw` - tr√°nh data leakage
- ‚úÖ Temporal ordering maintained (`sort_values('date')`)
- ‚úÖ Sliding window approach - maximize training samples
- ‚úÖ Validation checks (`if len(group) < self.sequence_length + 1`)

#### b) Normalization ‚≠ê‚≠ê‚≠ê‚≠ê
```python
self.scaler_mean = X_seq.mean(axis=(0, 1))  # Mean per feature
self.scaler_std = X_seq.std(axis=(0, 1)) + 1e-8  # +epsilon to avoid div by 0
X_seq = (X_seq - self.scaler_mean) / self.scaler_std
```

**Pros:**
- ‚úÖ Z-score normalization - standard practice
- ‚úÖ Epsilon `1e-8` prevents division by zero
- ‚úÖ Scaler parameters saved for inference

---

### 3. üîß **Code Quality & Engineering**

#### a) Error Handling ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```python
try:
    import torch
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    # Graceful fallback - code still runs!
```

**Excellent practices:**
- ‚úÖ Try-except for optional dependencies
- ‚úÖ Dummy classes to prevent NameError
- ‚úÖ Clear user messages when PyTorch unavailable

#### b) Model Persistence ‚≠ê‚≠ê‚≠ê‚≠ê
```python
def save_models(self, prefix='ensemble'):
    # Save LightGBM
    pickle.dump(self.lgbm_model, f)
    
    # Save LSTM
    torch.save(self.lstm_model.state_dict(), lstm_path)
    
    # Save config JSON
    json.dump(config, f)
```

**Good practices:**
- ‚úÖ Separate files for different components
- ‚úÖ Config saved as JSON - human readable
- ‚úÖ Scalers saved for reproducibility

#### c) Clean Code Structure ‚≠ê‚≠ê‚≠ê‚≠ê
- ‚úÖ Clear docstrings
- ‚úÖ Modular functions (train_lgbm, train_lstm, predict_ensemble)
- ‚úÖ Consistent naming conventions
- ‚úÖ Appropriate use of classes

---

### 4. üéì **Training Process**

#### a) Temporal Split ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
```python
split_date = df['date'].quantile(0.8)
train_df = df[df['date'] <= split_date]
val_df = df[df['date'] > split_date]
```

**Perfect for time series:**
- ‚úÖ NO random shuffling - respects temporal order
- ‚úÖ 80/20 split reasonable
- ‚úÖ Validation simulates future prediction

#### b) Validation on Both Models ‚≠ê‚≠ê‚≠ê‚≠ê
```python
# LightGBM validation
mae_val = mean_absolute_error(y_val, y_pred_val)

# LSTM validation
lstm_mae = ensemble.train_lstm(X_seq_train, y_seq_train, 
                                X_seq_val, y_seq_val)
```

---

## ‚ö†Ô∏è ƒêI·ªÇM C·∫¶N C·∫¢I THI·ªÜN (Areas for Improvement)

### 1. üö® **Critical Issues**

#### A. Overfitting Risk - No Regularization ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
```python
# HI·ªÜN T·∫†I: Ch·ªâ c√≥ dropout trong LSTM
self.lstm = nn.LSTM(dropout=0.2)

# V·∫§N ƒê·ªÄ:
- ‚ùå Kh√¥ng c√≥ weight decay trong optimizer
- ‚ùå Kh√¥ng c√≥ early stopping
- ‚ùå Kh√¥ng c√≥ learning rate scheduler
- ‚ùå Fixed 30-50 epochs - c√≥ th·ªÉ overfit ho·∫∑c underfit
```

**Khuy·∫øn ngh·ªã:**
```python
# 1. Add weight decay
optimizer = optim.Adam(
    self.lstm_model.parameters(), 
    lr=0.001,
    weight_decay=1e-5  # ‚Üê ADD THIS
)

# 2. Implement early stopping
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                return True  # Stop training
        else:
            self.best_loss = val_loss
            self.counter = 0
        return False

# Usage in train_lstm:
early_stopping = EarlyStopping(patience=10)
for epoch in range(max_epochs):
    # ... training ...
    if early_stopping(val_loss):
        print(f"Early stopping at epoch {epoch}")
        break

# 3. Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, 
    mode='min', 
    factor=0.5, 
    patience=5
)
# In training loop:
scheduler.step(val_loss)
```

---

#### B. Batch Training Missing ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
```python
# HI·ªÜN T·∫†I: Train tr√™n to√†n b·ªô dataset m·ªôt l√∫c
outputs = self.lstm_model(X_train_t)  # All data at once!

# V·∫§N ƒê·ªÄ:
- ‚ùå Memory issues v·ªõi large datasets
- ‚ùå Slower convergence
- ‚ùå No mini-batch SGD benefits
- ‚ùå Gradient accumulation over entire dataset
```

**Khuy·∫øn ngh·ªã:**
```python
from torch.utils.data import DataLoader, TensorDataset

def train_lstm_with_batches(self, X_seq_train, y_train, 
                            X_seq_val=None, y_val=None, 
                            epochs=100, batch_size=64):
    
    # Create DataLoader
    train_dataset = TensorDataset(
        torch.FloatTensor(X_seq_train),
        torch.FloatTensor(y_train)
    )
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True  # ‚úÖ Shuffle for better training
    )
    
    # Training loop
    for epoch in range(epochs):
        self.lstm_model.train()
        epoch_loss = 0
        
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = self.lstm_model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
        
        avg_loss = epoch_loss / len(train_loader)
        
        # Validation
        if X_seq_val is not None:
            val_loss = self.validate(X_seq_val, y_val)
            scheduler.step(val_loss)
            
            if early_stopping(val_loss):
                break
```

---

#### C. No Model Evaluation Metrics ‚ö†Ô∏è‚ö†Ô∏è
```python
# HI·ªÜN T·∫†I: Ch·ªâ print MAE
print(f"   Val MAE:   {mae_val:.2f}")

# V·∫§N ƒê·ªÄ:
- ‚ùå Kh√¥ng c√≥ R¬≤ score
- ‚ùå Kh√¥ng c√≥ RMSE
- ‚ùå Kh√¥ng c√≥ confidence intervals
- ‚ùå Kh√¥ng track training history
```

**Khuy·∫øn ngh·ªã:**
```python
from sklearn.metrics import r2_score, mean_squared_error

def comprehensive_evaluation(self, y_true, y_pred):
    """Evaluate model with multiple metrics"""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    # Percentage error
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    print(f"\n Evaluation Metrics:")
    print(f"   MAE:  {mae:.2f} points")
    print(f"   RMSE: {rmse:.2f} points")
    print(f"   R¬≤:   {r2:.4f}")
    print(f"   MAPE: {mape:.2f}%")
    
    return {
        'mae': mae,
        'rmse': rmse,
        'r2': r2,
        'mape': mape
    }
```

---

### 2. ‚ö° **Performance Issues**

#### A. Sequence Length Fixed at 12 ‚ö†Ô∏è
```python
# HI·ªÜN T·∫†I:
sequence_length = 12  # Fixed!

# V·∫§N ƒê·ªÄ:
- ‚ùå Kh√¥ng c√≥ ablation study
- ‚ùå 12 tu·∫ßn c√≥ th·ªÉ qu√° d√†i ho·∫∑c qu√° ng·∫Øn
- ‚ùå Kh√°c nhau cho t·ª´ng draw (WS vs MD)?
```

**Khuy·∫øn ngh·ªã:**
```python
# Test multiple sequence lengths
for seq_len in [6, 9, 12, 15, 18]:
    ensemble = EnsembleForecaster(sequence_length=seq_len)
    # Train and evaluate
    # Compare results
```

---

#### B. Hyperparameters Not Tuned ‚ö†Ô∏è‚ö†Ô∏è
```python
# HI·ªÜN T·∫†I: Hard-coded hyperparameters
hidden_size=64,
num_layers=2,
dropout=0.2,
lr=0.001

# V·∫§N ƒê·ªÄ:
- ‚ùå No grid search
- ‚ùå No random search
- ‚ùå No Bayesian optimization
```

**Khuy·∫øn ngh·ªã:**
```python
import optuna

def objective(trial):
    # Suggest hyperparameters
    hidden_size = trial.suggest_int('hidden_size', 32, 128, step=32)
    num_layers = trial.suggest_int('num_layers', 1, 3)
    dropout = trial.suggest_float('dropout', 0.1, 0.5)
    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)
    
    # Train model with suggested params
    ensemble = EnsembleForecaster()
    ensemble.lstm_model = LSTMModel(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=dropout
    )
    # ... training code ...
    
    return val_mae

# Run optimization
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)
print('Best hyperparameters:', study.best_params)
```

---

### 3. üîç **Missing Features**

#### A. No Visualization ‚ö†Ô∏è‚ö†Ô∏è
```python
# Thi·∫øu:
- ‚ùå Learning curves (loss over epochs)
- ‚ùå Predictions vs actuals plot
- ‚ùå Residuals analysis
- ‚ùå Feature importance (LSTM attention?)
```

**Khuy·∫øn ngh·ªã:**
```python
import matplotlib.pyplot as plt

# 1. Learning curves
def plot_learning_curves(train_losses, val_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Val Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('LSTM Learning Curves')
    plt.savefig('outputs/lstm_learning_curves.png')

# 2. Predictions vs Actual
def plot_predictions(y_true, y_pred, title='LSTM Predictions'):
    plt.figure(figsize=(10, 6))
    plt.scatter(y_true, y_pred, alpha=0.5)
    plt.plot([y_true.min(), y_true.max()], 
             [y_true.min(), y_true.max()], 
             'r--', label='Perfect prediction')
    plt.xlabel('Actual Points')
    plt.ylabel('Predicted Points')
    plt.legend()
    plt.title(title)
    plt.savefig(f'outputs/{title.lower().replace(" ", "_")}.png')

# 3. Residuals
def plot_residuals(y_true, y_pred):
    residuals = y_true - y_pred
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.scatter(y_pred, residuals, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Predicted Points')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    
    plt.subplot(1, 2, 2)
    plt.hist(residuals, bins=50, alpha=0.7)
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.title('Residual Distribution')
    
    plt.tight_layout()
    plt.savefig('outputs/lstm_residuals.png')
```

---

#### B. No Cross-validation ‚ö†Ô∏è
```python
# HI·ªÜN T·∫†I: Single train/val split
split_date = df['date'].quantile(0.8)

# V·∫§N ƒê·ªÄ:
- ‚ùå Results might be lucky/unlucky
- ‚ùå No confidence in performance estimates
- ‚ùå No robustness check
```

**Khuy·∫øn ngh·ªã:**
```python
from sklearn.model_selection import TimeSeriesSplit

def time_series_cv(df, n_splits=5):
    """Time series cross-validation"""
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    mae_scores = []
    
    for fold, (train_idx, val_idx) in enumerate(tscv.split(df)):
        print(f"\n Fold {fold + 1}/{n_splits}")
        
        train_df = df.iloc[train_idx]
        val_df = df.iloc[val_idx]
        
        # Train ensemble
        ensemble = EnsembleForecaster()
        # ... training code ...
        
        # Evaluate
        mae = evaluate(ensemble, val_df)
        mae_scores.append(mae)
    
    print(f"\n CV Results:")
    print(f"   Mean MAE: {np.mean(mae_scores):.2f} ¬± {np.std(mae_scores):.2f}")
    
    return mae_scores
```

---

#### C. No Attention Mechanism ‚ö†Ô∏è
```python
# HI·ªÜN T·∫†I: Basic LSTM - all timesteps treated equally
lstm_out, _ = self.lstm(x)
last_output = lstm_out[:, -1, :]  # Only use last timestep

# V·∫§N ƒê·ªÄ:
- ‚ùå Kh√¥ng bi·∫øt timestep n√†o quan tr·ªçng
- ‚ùå Recent weeks vs older weeks - no distinction
- ‚ùå Missing interpretability
```

**Khuy·∫øn ngh·ªã:**
```python
class AttentionLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        
        # Attention mechanism
        self.attention = nn.Linear(hidden_size, 1)
        self.fc = nn.Linear(hidden_size, 1)
    
    def forward(self, x):
        # LSTM output: (batch, seq_len, hidden_size)
        lstm_out, _ = self.lstm(x)
        
        # Attention weights
        attention_weights = torch.softmax(
            self.attention(lstm_out), 
            dim=1
        )  # (batch, seq_len, 1)
        
        # Weighted sum
        context = torch.sum(
            attention_weights * lstm_out, 
            dim=1
        )  # (batch, hidden_size)
        
        # Output
        output = self.fc(context)
        
        return output.squeeze(), attention_weights
```

---

### 4. üìä **Data Issues**

#### A. Ensemble Weight Hard-coded ‚ö†Ô∏è
```python
# HI·ªÜN T·∫†I:
self.lgbm_weight = 0.6
self.lstm_weight = 0.4

# V·∫§N ƒê·ªÄ:
- ‚ùå Arbitrary weights!
- ‚ùå Kh√¥ng test other ratios
- ‚ùå Should be learned or grid searched
```

**Khuy·∫øn ngh·ªã:**
```python
# Option 1: Grid search
best_mae = float('inf')
best_weights = (0.6, 0.4)

for lgbm_w in [0.5, 0.6, 0.7, 0.8]:
    lstm_w = 1 - lgbm_w
    
    pred = lgbm_w * lgbm_pred + lstm_w * lstm_pred
    mae = mean_absolute_error(y_val, pred)
    
    if mae < best_mae:
        best_mae = mae
        best_weights = (lgbm_w, lstm_w)

# Option 2: Learn weights (meta-model)
class LearnedEnsemble(nn.Module):
    def __init__(self):
        super().__init__()
        self.weight_lgbm = nn.Parameter(torch.tensor(0.6))
        self.weight_lstm = nn.Parameter(torch.tensor(0.4))
    
    def forward(self, lgbm_pred, lstm_pred):
        # Softmax to ensure weights sum to 1
        weights = torch.softmax(
            torch.stack([self.weight_lgbm, self.weight_lstm]), 
            dim=0
        )
        return weights[0] * lgbm_pred + weights[1] * lstm_pred
```

---

#### B. Feature Selection Not Optimized ‚ö†Ô∏è
```python
# HI·ªÜN T·∫†I: Manual feature selection
feature_cols = [
    'rank', 'tournaments_played',
    'points_lag_1', 'points_lag_3', 'points_lag_6',
    # ... 13 features total
]

# V·∫§N ƒê·ªÄ:
- ‚ùå Kh√¥ng test with more/less features
- ‚ùå No feature importance analysis for LSTM
- ‚ùå Same features for LightGBM and LSTM
```

**Khuy·∫øn ngh·ªã:**
```python
# 1. LightGBM feature importance
lgbm_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': ensemble.lgbm_model.feature_importances_
}).sort_values('importance', ascending=False)

print(lgbm_importance)

# 2. Try different feature sets for LSTM
temporal_features = [
    'points_lag_1', 'points_lag_3', 'points_lag_6',
    'avg_points_3m', 'avg_points_6m', 'momentum_score'
]

# LSTM might work better with only temporal features!
```

---

## üéØ ∆Øu Ti√™n C·∫£i Thi·ªán (Priority Improvements)

### üî¥ Critical (Ph·∫£i l√†m ngay)
1. **Add batch training** - Prevent memory issues
2. **Implement early stopping** - Prevent overfitting
3. **Add comprehensive metrics** (R¬≤, RMSE, MAPE)

### üü° High Priority (N√™n l√†m s·ªõm)
4. **Learning curves visualization** - Monitor training
5. **Hyperparameter tuning** - Improve performance
6. **Cross-validation** - Robust evaluation

### üü¢ Medium Priority (C√≥ th·ªÉ l√†m sau)
7. **Attention mechanism** - Better interpretability
8. **Learn ensemble weights** - Optimal combination
9. **Feature importance** - Better understanding
10. **Different sequence lengths** - Find optimal

### üîµ Low Priority (Nice to have)
11. **Bidirectional LSTM** - Better context
12. **GRU comparison** - Simpler alternative
13. **Transformer** - State-of-the-art

---

## üìà Expected Performance Improvement

N·∫øu implement c√°c c·∫£i thi·ªán tr√™n, expected gains:

| Improvement | Expected Œî MAE | Priority |
|-------------|----------------|----------|
| Early stopping + weight decay | -50 to -100 points | üî¥ Critical |
| Batch training + LR scheduler | -30 to -80 points | üî¥ Critical |
| Hyperparameter tuning | -100 to -200 points | üü° High |
| Attention mechanism | -50 to -150 points | üü° High |
| Learned ensemble weights | -20 to -80 points | üü¢ Medium |
| **Total estimated** | **-250 to -610 points** | - |

---

## üí° T·ªïng K·∫øt & Khuy·∫øn Ngh·ªã

### ‚úÖ L√†m t·ªët r·ªìi:
1. ‚ú® Architecture design (Ensemble)
2. ‚ú® Data preparation (sequences, normalization)
3. ‚ú® Code quality (modular, error handling)
4. ‚ú® Temporal split (no data leakage)

### ‚ö†Ô∏è C·∫ßn c·∫£i thi·ªán:
1. üîß Training process (batches, early stopping)
2. üîß Regularization (weight decay, LR scheduler)
3. üîß Evaluation (more metrics, CV, visualization)
4. üîß Hyperparameter tuning

### üéØ Next Steps:
```
Week 1: Critical fixes
  - Implement batch training
  - Add early stopping
  - Add comprehensive metrics

Week 2: Performance improvements
  - Hyperparameter tuning with Optuna
  - Cross-validation
  - Learning curves visualization

Week 3: Advanced features
  - Attention mechanism
  - Learn ensemble weights
  - Feature importance analysis
```

---

## üìö Code Examples - Quick Win Improvements

### 1. Complete train_lstm with all improvements:

```python
def train_lstm_improved(self, X_seq_train, y_train, X_seq_val, y_val,
                       epochs=100, batch_size=64, patience=10):
    """Improved LSTM training with all best practices"""
    
    if not PYTORCH_AVAILABLE:
        return None
    
    print("\n Training LSTM with improvements...")
    
    # Initialize model
    input_size = X_seq_train.shape[2]
    self.lstm_model = LSTMModel(
        input_size=input_size,
        hidden_size=64,
        num_layers=2,
        dropout=0.3  # Slightly higher dropout
    )
    
    # Loss, optimizer, scheduler
    criterion = nn.MSELoss()
    optimizer = optim.Adam(
        self.lstm_model.parameters(), 
        lr=0.001,
        weight_decay=1e-5  # ‚Üê Weight decay
    )
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    
    # DataLoader for batching
    train_dataset = TensorDataset(
        torch.FloatTensor(X_seq_train),
        torch.FloatTensor(y_train)
    )
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True
    )
    
    val_dataset = TensorDataset(
        torch.FloatTensor(X_seq_val),
        torch.FloatTensor(y_val)
    )
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    
    # Early stopping
    early_stopping = EarlyStopping(patience=patience)
    
    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_mae': []
    }
    
    # Training loop
    for epoch in range(epochs):
        # Train
        self.lstm_model.train()
        train_loss = 0
        
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = self.lstm_model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(
                self.lstm_model.parameters(), 
                max_norm=1.0
            )
            
            optimizer.step()
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        
        # Validation
        self.lstm_model.eval()
        val_loss = 0
        all_preds = []
        all_targets = []
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                outputs = self.lstm_model(batch_X)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
                
                all_preds.extend(outputs.numpy())
                all_targets.extend(batch_y.numpy())
        
        avg_val_loss = val_loss / len(val_loader)
        val_mae = mean_absolute_error(all_targets, all_preds)
        
        # Update history
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['val_mae'].append(val_mae)
        
        # Scheduler step
        scheduler.step(avg_val_loss)
        
        # Print progress
        if (epoch + 1) % 10 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"   Epoch [{epoch+1}/{epochs}]")
            print(f"     Train Loss: {avg_train_loss:.4f}")
            print(f"     Val Loss: {avg_val_loss:.4f}")
            print(f"     Val MAE: {val_mae:.2f}")
            print(f"     LR: {current_lr:.6f}")
        
        # Early stopping
        if early_stopping(avg_val_loss):
            print(f"\n   Early stopping at epoch {epoch+1}")
            break
    
    # Plot learning curves
    self.plot_learning_curves(history)
    
    # Final evaluation
    print(f"\n  Final Validation MAE: {val_mae:.2f}")
    
    return val_mae
```

---

## üèÜ K·∫øt Lu·∫≠n

**Overall Rating: 4/5 ‚≠ê‚≠ê‚≠ê‚≠ê**

Code c·ªßa b·∫°n ƒë√£ **r·∫•t t·ªët** cho starting point. C√≥ architecture design solid, data preparation excellent, v√† code quality cao. Tuy nhi√™n, c√≤n nhi·ªÅu improvement opportunities ƒë·ªÉ ƒë·∫°t production-level performance.

**V·ªõi c√°c c·∫£i thi·ªán ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t, expected performance improvement:**
- Current: ~800-1200 MAE (estimated)
- Improved: ~500-800 MAE (**30-40% reduction**)

H√£y b·∫Øt ƒë·∫ßu v·ªõi **Critical improvements** tr∆∞·ªõc nh√©! üöÄ
