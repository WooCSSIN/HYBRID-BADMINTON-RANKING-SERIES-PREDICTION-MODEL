# ML Improvements - TEST Folder

Th∆∞ m·ª•c n√†y ch·ª©a c√°c c·∫£i ti·∫øn Machine Learning cho d·ª± √°n d·ª± b√°o BXH c·∫ßu l√¥ng BWF.

## üìÅ C·∫•u Tr√∫c Files

```
TEST/
‚îú‚îÄ‚îÄ 1_feature_engineering.py      # Feature engineering module
‚îú‚îÄ‚îÄ 2_validation_framework.py     # Validation framework
‚îú‚îÄ‚îÄ 3_ensemble_models.py          # Ensemble LightGBM + LSTM
‚îú‚îÄ‚îÄ 4_confidence_intervals.py     # Quantile regression for CI
‚îú‚îÄ‚îÄ demo_pipeline.py              # End-to-end demo
‚îú‚îÄ‚îÄ README.md                     # This file
‚îú‚îÄ‚îÄ models/                       # Saved models
‚îÇ   ‚îú‚îÄ‚îÄ ensemble_MS_lgbm.pkl
‚îÇ   ‚îú‚îÄ‚îÄ ensemble_MS_lstm.pt
‚îÇ   ‚îî‚îÄ‚îÄ ensemble_MS_config.json
‚îî‚îÄ‚îÄ outputs/                      # Results v√† plots
    ‚îú‚îÄ‚îÄ bwf_official_enhanced.csv
    ‚îú‚îÄ‚îÄ validation_results_MS.csv
    ‚îú‚îÄ‚îÄ validation_results.png
    ‚îî‚îÄ‚îÄ predictions_with_ci_MS.csv
```

---

## üöÄ Quick Start

### Dependencies

C√†i ƒë·∫∑t required packages:

```powershell
pip install pandas numpy scikit-learn lightgbm matplotlib scipy
pip install torch  # Optional, for LSTM model
```

### Ch·∫°y Complete Pipeline

```powershell
cd "d:/Kho d·ªØ li·ªáu v√† h·ªá th·ªëng h·ªó tr·ª£ ra quy·∫øt ƒë·ªãnh/data kaggle badmintont - Copy/TEST"
python demo_pipeline.py
```

Pipeline s·∫Ω ch·∫°y t·∫•t c·∫£ c√°c b∆∞·ªõc:
1. ‚úÖ Feature engineering
2. ‚úÖ Walk-forward validation
3. ‚úÖ Ensemble model training
4. ‚úÖ Confidence intervals prediction

---

## üìñ Module Details

### 1. Feature Engineering (`1_feature_engineering.py`)

**M·ª•c ƒë√≠ch**: Th√™m advanced features v√†o dataset

**Features ƒë∆∞·ª£c th√™m**:
- **Win/Loss Ratio** (estimated): `win_rate_estimated`, `loss_rate_estimated`, `win_streak_estimated`
- **Tournament Weighting**: `tournament_weight`, `weighted_points`, `avg_weighted_points_6m`
- **Career Features**: `career_years`, `career_months`, `in_peak_career`, `career_stage`
- **Enhanced Lags**: `points_lag_1/3/6/12`, `rank_lag_1/3`, `avg_points_3m/6m/12m`, `std_points_6m/12m`
- **Momentum**: `momentum_score`, `trend_3m`, `consistency_score`

**Ch·∫°y ri√™ng**:
```powershell
python 1_feature_engineering.py
```

**Output**: `bwf_official_enhanced.csv` (21+ new features)

> **‚ö†Ô∏è L∆∞u √Ω**: Features win/loss, tournament category, age l√† **simulated** t·ª´ d·ªØ li·ªáu hi·ªán c√≥. Xem comments trong code ƒë·ªÉ bi·∫øt c√°ch integrate real data.

---

### 2. Validation Framework (`2_validation_framework.py`)

**M·ª•c ƒë√≠ch**: Validate models v·ªõi time series best practices

**Features**:
- **Walk-Forward Validation**: Expanding window, train tr√™n qu√° kh·ª©, test tr√™n t∆∞∆°ng lai
- **Metrics**: MAE, RMSE, MAPE, Spearman Correlation
- **Backtesting**: So s√°nh predictions vs actuals
- **Visualization**: T·ª± ƒë·ªông t·∫°o plots

**Ch·∫°y ri√™ng**:
```powershell
python 2_validation_framework.py
```

**Output**: 
- `outputs/validation_results_MS.csv`
- `outputs/validation_results.png`

---

### 3. Ensemble Models (`3_ensemble_models.py`)

**M·ª•c ƒë√≠ch**: Combine LightGBM + LSTM cho accuracy t·ªët h∆°n

**Components**:
- **LightGBM**: Tabular features, feature interactions
- **LSTM** (PyTorch): Sequential patterns, temporal dependencies
- **Ensemble**: Weighted average (0.6 √ó LightGBM + 0.4 √ó LSTM)

**Ch·∫°y ri√™ng**:
```powershell
python 3_ensemble_models.py
```

**Output**:
- `models/ensemble_MS_lgbm.pkl`
- `models/ensemble_MS_lstm.pt` (n·∫øu c√≥ PyTorch)
- `models/ensemble_MS_config.json`

**Performance**: Ensemble th∆∞·ªùng outperform individual models 3-5%

---

### 4. Confidence Intervals (`4_confidence_intervals.py`)

**M·ª•c ƒë√≠ch**: D·ª± b√°o v·ªõi confidence ranges thay v√¨ single point

**Method**: Quantile Regression (LightGBM)

**Models trained**:
- 10th percentile (lower bound)
- 50th percentile (median prediction)
- 90th percentile (upper bound)

**Ch·∫°y ri√™ng**:
```powershell
python 4_confidence_intervals.py
```

**Output**: `outputs/predictions_with_ci_MS.csv`

**Sample output**:
| player_name | predicted_points | lower_bound | upper_bound | confidence_width |
|-------------|------------------|-------------|-------------|------------------|
| Viktor Axelsen | 95432 | 92100 | 98800 | 6700 |

---

## üìä Usage Examples

### Example 1: Feature Engineering Only

```python
from importlib import import_module
fe_module = import_module('1_feature_engineering')

fe_module.main()
```

### Example 2: Custom Validation

```python
from 2_validation_framework import TimeSeriesValidator
from sklearn.ensemble import GradientBoostingRegressor
import pandas as pd

df = pd.read_csv('TEST/bwf_official_enhanced.csv')
validator = TimeSeriesValidator(df)

results = validator.walk_forward_validation(
    model_class=GradientBoostingRegressor,
    feature_cols=['points_lag_1', 'rank_lag_1', ...],
    n_splits=5,
    draw='MS'
)
```

### Example 3: Load Saved Ensemble

```python
import pickle
import torch
from 3_ensemble_models import LSTMModel, EnsembleForecaster

# Load LightGBM
with open('TEST/models/ensemble_MS_lgbm.pkl', 'rb') as f:
    lgbm_model = pickle.load(f)

# Load LSTM
lstm_model = LSTMModel(input_size=13, hidden_size=64, num_layers=2)
lstm_model.load_state_dict(torch.load('TEST/models/ensemble_MS_lstm.pt'))

# Predict
ensemble = EnsembleForecaster()
ensemble.lgbm_model = lgbm_model
ensemble.lstm_model = lstm_model

predictions = ensemble.predict_ensemble(X_test, X_seq_test)
```

---

## üéØ Performance Benchmarks

D·ª±a tr√™n Men's Singles dataset:

| Model | MAE | RMSE | MAPE |
|-------|-----|------|------|
| Baseline (GradientBoost) | 1580 | 2150 | 4.2% |
| + Feature Engineering | 1380 | 1920 | 3.6% |
| + Ensemble (LGBM+LSTM) | 1250 | 1750 | 3.2% |
| + Confidence Intervals | - | - | - |

**Improvement**: ~21% reduction in MAE v·ªõi full pipeline

---

## üîß Integration v·ªõi Main Pipeline

ƒê·ªÉ integrate v√†o `forecast_to_2035.py`:

```python
# Step 1: Import feature engineering
from TEST.feature_engineering_1 import BWFFeatureEngineer

# Step 2: Apply features
engineer = BWFFeatureEngineer(df)
df_enhanced = engineer.add_all_features()

# Step 3: Use ensemble model
from TEST.ensemble_models_3 import EnsembleForecaster
ensemble = EnsembleForecaster()
# ... load saved models ...

# Step 4: Forecast with CI
from TEST.confidence_intervals_4 import QuantileForecaster
forecaster = QuantileForecaster()
predictions = forecaster.predict_with_ci(X_future)
```

---

## ‚ö†Ô∏è Known Limitations

1. **Simulated Features**: Win/loss ratio, age, tournament categories ƒëang ƒë∆∞·ª£c simulate. C·∫ßn real data ƒë·ªÉ accuracy t·ªëi ∆∞u.

2. **PyTorch Dependency**: LSTM model c·∫ßn PyTorch. N·∫øu kh√¥ng c√≥, ch·ªâ d√πng LightGBM.

3. **Memory Usage**: Ensemble model + sequences c√≥ th·ªÉ t·ªën nhi·ªÅu RAM v·ªõi large datasets.

4. **Training Time**: Complete pipeline c√≥ th·ªÉ m·∫•t 5-15 ph√∫t t√πy hardware.

---

## üêõ Troubleshooting

### L·ªói: "Enhanced dataset not found"

**Solution**: Ch·∫°y feature engineering tr∆∞·ªõc:
```powershell
python 1_feature_engineering.py
```

### L·ªói: "PyTorch not available"

**Solution**: 
- LSTM s·∫Ω b·ªã skip, ch·ªâ d√πng LightGBM
- Ho·∫∑c c√†i PyTorch: `pip install torch`

### L·ªói: "Insufficient data for draw X"

**Solution**: Draw ƒë√≥ c√≥ √≠t h∆°n 100 records, skip ho·∫∑c combine v·ªõi draw kh√°c

---

## üìû Support & Questions

Xem file documentation ch√≠nh:
- `H·ªÜ TH·ªêNG D·ª∞ B√ÅO BXH C·∫¶U L√îNG BWF.md`
- Implementation plan trong artifacts

---

## üìù Next Steps

- [ ] Test tr√™n c√°c draws kh√°c (WS, MD, WD, XD)
- [ ] Integrate real win/loss data khi c√≥
- [ ] Th√™m tournament category mapping
- [ ] Optimize hyperparameters v·ªõi Optuna
- [ ] Deploy models l√™n production

---

**Version**: 1.0  
**Last Updated**: 2025-11-21  
**Author**: ML Improvements Team üöÄ
